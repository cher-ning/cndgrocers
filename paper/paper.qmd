---
title: "My title"
subtitle: "My subtitle if needed"
author: Cher Ning-Li
thanks: "Code and data are available at: [https://github.com/cher-ning/cndgrocers)."
date: today
date-format: long
abstract: "First sentence. Second sentence. Third sentence. Fourth sentence."
format: pdf
number-sections: true
bibliography: references.bib
---

```{r}
#| echo: false
#| eval: true
#| warning: false
#| message: false

library(tidyverse)
library(arrow)
library(here)
library(ggplot2)
library(modelsummary)
library(rstanarm)
library(knitr)

# Read in analysis_data
analysis_data <- read_parquet(file = here("data/02-analysis_data/analysis_data.parquet"), show_col_types = FALSE)

# Read in models
model1 <- readRDS(file = here("models/bayes_model1.rds"))
model2 <- readRDS(file = here("models/bayes_model2.rds"))
```


# Introduction

#TODO: Overview paragraph

The estimand that is targetted within this paper's analysis is the price of a dozen eggs across the 8 top Canadian grocers. 

#TODO: Results paragraph

#TODO: Why it matters paragraph

#TODO: Telegraphing paragraph: The remainder of this paper is structured as follows. @sec-data....




# Data {#sec-data}

## Overview

The dataset used for this paper's analysis is titled "Canadian Grocery Price Data" and was obtained from Project Hammer [@projecthammer], which compiles grocery store price listings across the 8 major Canadian Grocers â€” Voila, T&T, Loblaws, No Frills, Metro, Galleria, Walmart, and Save-On-Foods. The dataset contains data starting from February 28, 2024 and is updated regularly with the addition of new entries. Only the entries up until November 25, 2024 are included within this paper's analysis.

The dataset was downloaded as two separate csv files titled `hammer-4-product.csv` and `hammer-4-raw.csv`. The `product` table includes information about specific products that are available across the different grocers, with unique product ID numbers to identify each listing at each grocer. The `raw` table includes the price and more time-specific information regarding each product's listing. The two tables were joined by matching the product IDs to create the full dataset, a comprehensive table where each entry holds all the necessary product and listing information. 

Under the guidance of @tellingstories, the R programming language [@citeR] was used for analysis of this dataset. The package `tidyverse` [@tidyverse] was used to simulate data and test the simulated data before analysis. Packages `tidyverse` [@tidyverse] and `arrow` [@arrow] were utilized to clean the full raw dataset,  which were then also used alongside packages `testthat` [@testthat] and `here` [@here] to test the cleaned dataset. Lastly, packages `tidyverse` [@tidyverse], `rstanarm` [@rstanarm], `arrow` [@arrow], and `here` [@here] were used to build predictive models using the cleaned dataset. 

## Measurement

The observations in the dataset are scraped from the grocer's official website's interface, which means it only contains the information that is publically listed and available. The recorded prices for each item are the listed price for the "in store pick up" option, with the target pickup neighbourhood being a neighbourhood in Toronto. Certain values and listings may be missed, since the internal APIs that power the grocer's websites are not accessed and specific extracts may error for certain vendors on particular days for unforeseeable reasons. Starting from July 11, the targetted variety of grocery items for data collection was greatly increased, meaning that there may be products which are missing pricing information before then.

## Data-Cleaning

After combining the two tables, `raw` and `product`, by matching each entry's unique product ID, the full dataset with $12,753,964$ entries was created. Then, entries with "egg" or "eggs", and "dozen" or "12" appearing in their product name or units were selected for. The `nowtime` variable, which tracks the time when the data was scraped, was generalized to a `month` variable for ease of analysis. All February entries were omitted from analysis due to data collection starting on 28 February, 2024, which means there is a lack of sufficient data for that month to make any reliable conclusions from. Then, calculation for each month's average `current_price` was done and added to each listing of the following month as the variable `prev_month_avg`. 

The relevant variables, as listed and explained in the table below [@tbl-varoverview], was selected for and entries with empty entries in any of these key variables were dropped. It is important to note that this would filter out any unadvertised price changes, as only price decreases for an explicitly advertised sale event would include both `current_price` and `old_price`.

```{r}
#| label: tbl-varoverview
#| tbl-cap: Selected Variables Overview
#| echo: false
#| warning: false
#| message: false

columns <- data.frame(
  Column = c("month", "vendor", "current_price", "old_price", "product_name", "prev_month_avg"),
  Description = c(
    "Month of when the data was collected",
    "One of the 8 Canadian grocery vendors",
    "Price at time of extract",
    "An 'old' struck-out price, indicating the item's regular pre-sale price",
    "Product name, may include brand and/or units",
    "Average curren_price of a dozen eggs across all vendors, during the month before the data collection"
  )
)

kable(columns, caption = NULL)

```

A snapshot of a couple entries from this cleaned dataset can be found in @sec-data-details.

## Outcome variables

Add graphs, tables and text. Use sub-sub-headings for each outcome variable or update the subheading to be singular.

```{r}
#| label: fig-planes
#| fig-cap: Relationship between wing length and width
#| echo: false
#| warning: false
#| message: false

analysis_data |> 
  ggplot(aes(x = width, y = length)) +
  geom_point(alpha = 0.8) +
  theme_minimal() +
  labs(x = "Wing width (mm)",
       y = "Wing length (mm)")
```

Talk way more about it. 

## Predictor variables

Add graphs, tables and text.

Use sub-sub-headings for each outcome variable and feel free to combine a few into one if they go together naturally.




# Model

The goal of our modelling strategy is twofold. Firstly,...

Here we briefly describe the Bayesian analysis model used to investigate... Background details and diagnostics are included in [Appendix -@sec-model-details].

## Model set-up

Define $y_i$ as the number of seconds that the plane remained aloft. Then $\beta_i$ is the wing width and $\gamma_i$ is the wing length, both measured in millimeters.  

\begin{align} 
y_i|\mu_i, \sigma &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_i + \gamma_i\\
\alpha &\sim \mbox{Normal}(0, 2.5) \\
\beta &\sim \mbox{Normal}(0, 2.5) \\
\gamma &\sim \mbox{Normal}(0, 2.5) \\
\sigma &\sim \mbox{Exponential}(1)
\end{align}

We run the model in R [@citeR] using the `rstanarm` package of @rstanarm. We use the default priors from `rstanarm`.


### Model justification

We expect a positive relationship between the size of the wings and time spent aloft. In particular...

We can use maths by including latex between dollar signs, for instance $\theta$.


# Results

Our results are summarized in @tbl-modelresults.


```{r}
#| echo: false
#| eval: true
#| label: tbl-modelresults
#| tbl-cap: "Explanatory models of flight time based on wing width and wing length"
#| warning: false

modelsummary::modelsummary(
  list(
    "First model" = model1
  ),
  statistic = "mad",
  fmt = 2
)
```


# Discussion

## First discussion point {#sec-first-point}

If my paper were 10 pages, then should be be at least 2.5 pages. The discussion is a chance to show off what you know and what you learnt from all this. 

## Second discussion point

Please don't use these as sub-heading labels - change them to be what your point actually is.

## Third discussion point

## Weaknesses and next steps

Weaknesses and next steps should also be included.
- maybe should try to specify the prev_month_avg down to also selection by vendor

\newpage

\appendix

# Appendix {-}


# Additional data details {#sec-data-details}

!!! ADD FIRST 5 ROWS DATA HERE

# Model details {#sec-model-details}

## Posterior predictive check

In @fig-ppcheckandposteriorvsprior-1 we implement a posterior predictive check. This shows...

In @fig-ppcheckandposteriorvsprior-2 we compare the posterior with the prior. This shows... 

```{r}
#| eval: true
#| echo: false
#| message: false
#| warning: false
#| label: fig-ppcheckandposteriorvsprior
#| layout-ncol: 2
#| fig-cap: "Examining how the model fits, and is affected by, the data"
#| fig-subcap: ["Posterior prediction check", "Comparing the posterior with the prior"]

pp_check(model1) +
  theme_classic() +
  theme(legend.position = "bottom")

posterior_vs_prior(model1) +
  theme_minimal() +
  scale_color_brewer(palette = "Set1") +
  theme(legend.position = "bottom") +
  coord_flip()
```

## Diagnostics

@fig-stanareyouokay-1 is a trace plot. It shows... This suggests...

@fig-stanareyouokay-2 is a Rhat plot. It shows... This suggests...

```{r}
#| echo: false
#| eval: true
#| message: false
#| warning: false
#| label: fig-stanareyouokay
#| fig-cap: "Checking the convergence of the MCMC algorithm"
#| fig-subcap: ["Trace plot", "Rhat plot"]
#| layout-ncol: 2

plot(first_model, "trace")

plot(first_model, "rhat")
```



\newpage


# References


